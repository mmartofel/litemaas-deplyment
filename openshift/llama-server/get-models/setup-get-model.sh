#!/bin/bash

# This is an example script demonstrating how to get and set up models at ollama server deployed in OpenShift.
# Make sure you have 'oc' CLI tool installed and configured to access your OpenShift cluster.
# Also, ensure 'jq' is installed for JSON processing.
# Prerequisite: The Llama server should be deployed and accessible.
# Login to OpenShift cluster with admin credentials before running this script.

# Check if llama server is running, if not, wait until it is running

echo
echo "‚è≥ Checking Llama server to be ready..."
while true; do
  LLAMA_POD_STATUS=$(oc get pods -n litemaas -l app=llama-server -o jsonpath='{.items[0].status.phase}')
  if [ "$LLAMA_POD_STATUS" == "Running" ]; then
    echo "‚úÖ Llama server is running."
    break
  fi
  echo "‚è≥ Llama server is not ready yet. Waiting..."
  sleep 5
done

# üîç Get external route
LLAMA_HOST=$(oc get route llama -n litemaas -o jsonpath='{.spec.host}')

echo
echo "üåê LLAMA HOST: $LLAMA_HOST"
echo

# üì• Pull models
echo "‚¨áÔ∏è  Pulling models..."
curl -s "https://$LLAMA_HOST/api/pull" -d '{"model":"granite4"}'
curl -s "https://$LLAMA_HOST/api/pull" -d '{"model":"llama2"}'
curl -s "https://$LLAMA_HOST/api/pull" -d '{"model":"mistral"}'

echo
echo "üì¶ Available models:"
echo

# üß† List models with icons
curl -s "https://$LLAMA_HOST/api/tags" \
  | jq -r '.models[].name' \
  | sed 's/^/üß© /'

echo
echo "‚úÖ Model setup complete."
echo